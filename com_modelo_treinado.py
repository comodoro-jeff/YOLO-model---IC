# -*- coding: utf-8 -*-
"""Com_modelo_treinado.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jwkezbla4DBg88Sz7loLnriuSjfxKFYu
"""

!pip install ultralytics

from ultralytics import YOLO
import cv2

# Load the model YOLOv8
model = YOLO('/content/best_v2.pt')

from google.colab import drive
drive.mount('/content/drive')

import cv2
import numpy as np
from ultralytics import YOLO
from collections import defaultdict
from google.colab.patches import cv2_imshow

# Load the trained YOLOv8 model
# model = YOLO('/content/ic-2/runs/train/weights/best.pt')

# Define the video source
VIDEO_SOURCE = cv2.VideoCapture('/content/Video2.mp4')

# Get FPS and video duration
fps = VIDEO_SOURCE.get(cv2.CAP_PROP_FPS)
total_frames = int(VIDEO_SOURCE.get(cv2.CAP_PROP_FRAME_COUNT))
video_duration = total_frames / fps  # Total duration in seconds
frame_time = 1 / fps  # Time per frame

# Dictionary to track time per class
time_tracker = defaultdict(lambda: 0.0)
total_detected_time = 0.0  # Total detection time

# Batch size for frames
batch_size = 5
frames_batch = []

# Process the video in batches
while True:
    ret, frame = VIDEO_SOURCE.read()
    if not ret:
        break

    # Add frame to batch
    frames_batch.append(frame)

    # Process the batch when it reaches the defined size
    if len(frames_batch) >= batch_size:
        results = model(frames_batch)  # Perform batch prediction

        # Process each frame in the batch
        for i, result in enumerate(results):
            frame = frames_batch[i].copy()
            bboxes = np.array(result.boxes.xyxy.cpu(), dtype="int")
            classes = np.array(result.boxes.cls.cpu(), dtype="int")
            confidence = np.array(result.boxes.conf.cpu(), dtype="float")

            detected_classes = set()

            for cls, bbox, conf in zip(classes, bboxes, confidence):
                if conf < 0.5:  # Filter low-confidence detections
                    continue

                object_name = model.names[cls]
                detected_classes.add(object_name)

                # Update tracked time per class
                time_tracker[object_name] += frame_time

                # Draw bounding box and label
                cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (37, 245, 75), 2)
                cv2.putText(frame, f"{object_name}: {conf:.2f}", (bbox[0], bbox[1] - 5),
                            cv2.FONT_HERSHEY_PLAIN, 2, (37, 245, 75), 2)

            # Display the processed frame
            cv2_imshow(frame)

        # Clear the batch of frames
        frames_batch = []

# Calculate time without detection
time_no_detection = video_duration - sum(time_tracker.values())

# Convert seconds to minutes
def format_time(seconds, total):
    minutes = seconds / 60  # Convert to minutes
    percent = (seconds / total) * 100  # Calculate percentage
    return f"{minutes:.2f} minutes ({percent:.0f}%)"

# Display final results
print("\nTotal time per detected class:")
for obj, t in time_tracker.items():
    print(f"Time {obj}: {format_time(t, video_duration)}")

print(f"Time with no user detected: {format_time(time_no_detection, video_duration)}")
print(f"Total duration: {format_time(video_duration, video_duration)}")

#webcam

#codigo Webcam

# Definir a fonte de vídeo como a webcam
VIDEO_SOURCE = cv2.VideoCapture(0)  # 0 para a webcam padrão

# Obter FPS da webcam (pode variar)
fps = VIDEO_SOURCE.get(cv2.CAP_PROP_FPS)
frame_time = 1 / fps if fps > 0 else 1 / 30  # Prevenir divisão por zero

time_tracker = defaultdict(lambda: 0.0)
total_detected_time = 0.0  # Tempo total de detecção

while True:
    ret, frame = VIDEO_SOURCE.read()
    if not ret:
        break

    # Fazer a previsão com YOLO
    results = model(frame)

    # Processar os resultados
    for result in results:
        bboxes = np.array(result.boxes.xyxy.cpu(), dtype="int")
        classes = np.array(result.boxes.cls.cpu(), dtype="int")
        confidence = np.array(result.boxes.conf.cpu(), dtype="float")

        detected_classes = set()

        for cls, bbox, conf in zip(classes, bboxes, confidence):
            if conf < 0.5:  # Filtrar detecções com baixa confiança
                continue

            object_name = model.names[cls]
            detected_classes.add(object_name)
            time_tracker[object_name] += frame_time

            # Desenhar caixa e rótulo
            cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (37, 245, 75), 2)
            cv2.putText(frame, f"{object_name}: {conf:.2f}", (bbox[0], bbox[1] - 5),
                        cv2.FONT_HERSHEY_PLAIN, 2, (37, 245, 75), 2)

    # Mostrar o frame processado
    cv2.imshow("Webcam Detection", frame)

    # Pressione 'q' para sair
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Liberar os recursos
VIDEO_SOURCE.release()
cv2.destroyAllWindows()

# Exibir resultados finais
def format_time(seconds):
    minutes = seconds / 60
    return f"{minutes:.2f} minutos"

print("\nTempo total por classe detectada:")
for obj, t in time_tracker.items():
    print(f"Tempo {obj}: {format_time(t)}")